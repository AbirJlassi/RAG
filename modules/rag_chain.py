# üìÅ modules/rag_chain.py

from dotenv import load_dotenv
import os
from langchain.text_splitter import RecursiveCharacterTextSplitter
from modules.vectorstore import create_vectorstore
from modules.loader import load_and_tag_documents
from modules.llm import get_llm
from modules.storage import store_generation

load_dotenv()

# Chargement + enrichissement des documents
docs = load_and_tag_documents("data/")

# Chunking avec split logique
splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)
chunks = splitter.split_documents(docs)

# Cr√©ation du vecteur index
vectorstore = create_vectorstore(chunks)
retriever = vectorstore.as_retriever(search_kwargs={"k": 4})
llm = get_llm()

def generate_response(query, filters=None):
    """
    G√©n√®re une r√©ponse propale enrichie et structur√©e, avec tracking pour apprentissage continu
    
    Args:
        query (str): Question/demande de l'utilisateur
        filters (dict, optional): Filtres taxonomiques (secteur, domaine, etc.)
    """
    try:
        # Configuration de la recherche avec filtres
        search_kwargs = {"k": 4}
        
        # Application des filtres si fournis
        if filters:
            # Conversion des filtres en crit√®res de recherche
            filter_conditions = {}
            for key, value in filters.items():
                if value:  # Ignore les valeurs vides
                    filter_conditions[key] = value
            
            if filter_conditions:
                search_kwargs["filter"] = filter_conditions

        # Recherche des chunks pertinents (avec ou sans filtres)
        context_docs = retriever.get_relevant_documents(query)
        
        # Filtrage post-recherche si n√©cessaire (fallback)
        if filters and context_docs:
            filtered_docs = []
            for doc in context_docs:
                doc_metadata = doc.metadata
                match = True
                for key, value in filters.items():
                    if value and doc_metadata.get(key) != value:
                        match = False
                        break
                if match:
                    filtered_docs.append(doc)
            
            # Utiliser les docs filtr√©s si disponibles, sinon garder tous
            context_docs = filtered_docs if filtered_docs else context_docs
        
        if not context_docs:
            return "Aucun contenu pertinent trouv√© dans la base de connaissance."

        # Construction du contexte et m√©tadonn√©es
        context = "\n\n".join([doc.page_content for doc in context_docs])
        tags = context_docs[0].metadata if context_docs else {}
        
        # Enrichissement avec les filtres appliqu√©s
        if filters:
            tags.update({"filtres_appliques": filters})

        # Construction du prompt professionnel structur√©
        filter_info = f"\nüéØ Filtres appliqu√©s : {filters}" if filters else ""
        
        prompt = f"""
Tu es le consultant expert virtuel de l'entreprise SKILLIA, charg√© de g√©n√©rer une proposition commerciale structur√©e, √©crite en FRANCAIS et pas en anglais,  √† partir des documents internes de l‚Äôentreprise.

üìå Demande utilisateur :
{query}{filter_info}

üìö Contexte extrait :
{context}

üîñ M√©tadonn√©es associ√©es (secteur, domaine, sous-domaine, livrables, client, dur√©e, TJM) :
{tags}

‚úçÔ∏è La propale doit inclure :
1. Contexte client
2. Objectifs et enjeux identifi√©s
3. D√©marche ou m√©thodologie recommand√©e (avec r√©f√©rences √† la taxonomie si possible)
4. Livrables attendus ou livrables similaires observ√©s
5. Planning estim√© (phases, charges)
6. Budget indicatif ou TJM (si d√©tect√©)
7. Valeur ajout√©e de l'approche propos√©e

üß© Utilise la taxonomie interne (domaines, livrables, m√©thodes) pour structurer au mieux ta r√©ponse.
R√©dige en langage clair, professionnel et adapt√© au secteur d'activit√© du client.
"""

        response = llm.invoke(prompt)

        # Stockage pour apprentissage continu (feedforward)
        store_generation(
            query=query, 
            context=context, 
            metadata={**tags, **(filters or {})}, 
            response=response
        )

        return response

    except Exception as e:
        return f"‚ùå Erreur dans generate_response: {str(e)}"