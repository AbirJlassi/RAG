import os
from dotenv import load_dotenv
from sentence_transformers import CrossEncoder

from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.retrievers.multi_query import MultiQueryRetriever

from modules.vectorstore import create_vectorstore, create_bm25_vectorstore
from modules.loader import load_and_tag_documents
from modules.llm import get_llm
from modules.storage import store_generation
from modules.metrics import evaluate_rag

# üîê Charger les variables d'environnement (.env)
load_dotenv()

# üéØ Reranker CrossEncoder
reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')

# üì• Charger et enrichir les documents
docs = load_and_tag_documents("data/")

# ‚úÇÔ∏è Chunking des documents (chunks plus courts pour plus de pr√©cision)
splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
chunks = splitter.split_documents(docs)

# üß† Cr√©ation des vecteurs FAISS (denses) et BM25 (sparse)
faiss_vectorstore = create_vectorstore(chunks)
bm25_vectorstore = create_bm25_vectorstore(chunks)

# üîç Cr√©ation des retrievers
retriever_faiss = faiss_vectorstore.as_retriever(search_kwargs={"k": 8})
retriever_bm25 = bm25_vectorstore

# üîÅ Fusion manuelle des r√©sultats FAISS + BM25
def hybrid_retrieve(query, retriever1, retriever2, top_k=8):
    results1 = retriever1.get_relevant_documents(query)
    results2 = retriever2.get_relevant_documents(query)

    # Fusionner les documents sans doublons
    seen = set()
    combined = []
    for doc in results1 + results2:
        if doc.page_content not in seen:
            seen.add(doc.page_content)
            combined.append(doc)
        if len(combined) >= top_k:
            break
    return combined

# ü§ñ Chargement du mod√®le LLM
llm = get_llm()

# üöÄ Fonction principale : g√©n√®re r√©ponse + √©value la qualit√©
def generate_response(query, filters=None):
    """
    G√©n√®re une propale structur√©e √† partir des documents internes,
    puis √©value la qualit√© de la r√©ponse RAG.
    """
    try:
        # === 1. Application √©ventuelle des filtres ===
        search_kwargs = {"k": 8}
        if filters:
            filter_conditions = {k: v for k, v in filters.items() if v}
            if filter_conditions:
                search_kwargs["filter"] = filter_conditions

        # === 2. R√©cup√©ration hybride FAISS + BM25 ===
        context_docs = hybrid_retrieve(query, retriever_faiss, retriever_bm25, top_k=8)

        # === 3. Reranking avec CrossEncoder ===
        if context_docs:
            pairs = [[query, doc.page_content] for doc in context_docs]
            scores = reranker.predict(pairs)
            context_docs = [doc for _, doc in sorted(zip(scores, context_docs), key=lambda x: -x[0])]

        # === 4. Filtrage post-retrieval (fallback) ===
        if filters:
            filtered_docs = [
                doc for doc in context_docs
                if all(doc.metadata.get(k) == v for k, v in filters.items() if v)
            ]
            context_docs = filtered_docs if filtered_docs else context_docs

        if not context_docs:
            return "Aucun contenu pertinent trouv√© dans la base de connaissance."

        # === 5. Construction du contexte ===
        context = "\n\n".join([doc.page_content for doc in context_docs])
        tags = context_docs[0].metadata if context_docs else {}
        if filters:
            tags.update({"filtres_appliques": filters})

        # === 6. Prompt pour g√©n√©ration de propale ===
        filter_info = f"\nüéØ Filtres appliqu√©s : {filters}" if filters else ""
        prompt = f"""
Tu es le consultant expert virtuel de l'entreprise SKILLIA, charg√© de g√©n√©rer une proposition commerciale structur√©e, √©crite en FRANCAIS √† partir des documents internes de l‚Äôentreprise.

üìå Demande utilisateur :
{query}{filter_info}

üìö Contexte extrait :
{context}

üîñ M√©tadonn√©es associ√©es :
{tags}

‚úçÔ∏è La propale doit inclure :
1. Contexte client
2. Objectifs et enjeux identifi√©s
3. D√©marche ou m√©thodologie recommand√©e
4. Livrables attendus
5. Planning estim√©
6. Budget indicatif ou TJM
7. Valeur ajout√©e de l'approche propos√©e

üß© Utilise la taxonomie interne pour structurer au mieux ta r√©ponse.
"""

        # === 7. G√©n√©ration de r√©ponse via LLM ===
        response = llm.invoke(prompt).content

        # === 8. Sauvegarde de la g√©n√©ration ===
        store_generation(
            query=query,
            context=context,
            metadata={**tags, **(filters or {})},
            response=response
        )

        # === 9. √âvaluation RAG (qualit√© du retrieval et de la g√©n√©ration) ===
        reference_docs = []  # Mettre ici tes documents de v√©rit√© si disponibles
        retrieved_texts = [doc.page_content for doc in context_docs]

        metrics_report = evaluate_rag(
            query=query,
            generated_answer=response,
            retrieved_docs=retrieved_texts,
            reference_docs=reference_docs
        )

        # ‚úÖ 10. Retourner la r√©ponse + les m√©triques
        return {
            "response": response,
            "metrics": metrics_report
        }

    except Exception as e:
        return f"‚ùå Erreur dans generate_response: {str(e)}"
